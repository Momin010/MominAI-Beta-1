// ðŸš€ MOMINAI REVOLUTION - MULTI-MODEL AI SERVICE
// CRUSHING COMPETITORS WITH SUPPORT FOR ALL MAJOR AI MODELS

import OpenAI from 'openai'
import Anthropic from '@anthropic-ai/sdk'
import { GoogleGenerativeAI } from '@google/generative-ai'
import type { AIModel, AIProvider } from '@/types'

// SUPERIOR TO CURSOR: Support for ALL AI models, not just one
// SUPERIOR TO WINDSURF: No credit limits, unlimited usage
// SUPERIOR TO COPILOT: Multiple providers, not locked to Microsoft

export interface AIRequest {
  model: AIModel
  messages: Array<{
    role: 'system' | 'user' | 'assistant'
    content: string
  }>
  temperature?: number
  maxTokens?: number
  stream?: boolean
  context?: {
    files?: Array<{ path: string; content: string }>
    project?: string
    language?: string
  }
}

export interface AIResponse {
  content: string
  model: AIModel
  usage: {
    inputTokens: number
    outputTokens: number
    totalTokens: number
  }
  duration: number
  cached: boolean
}

export class MultiModelAIService {
  private openai: OpenAI | null = null
  private anthropic: Anthropic | null = null
  private google: GoogleGenerativeAI | null = null
  
  private providers: Map<string, AIProvider> = new Map()
  private cache: Map<string, AIResponse> = new Map()
  private rateLimiter: Map<string, { requests: number; resetTime: number }> = new Map()

  constructor() {
    this.initializeProviders()
  }

  private initializeProviders() {
    // OpenAI Provider
    if (process.env.OPENAI_API_KEY) {
      this.openai = new OpenAI({
        apiKey: process.env.OPENAI_API_KEY,
        dangerouslyAllowBrowser: true
      })

      this.providers.set('openai', {
        id: 'openai',
        name: 'OpenAI',
        models: ['gpt-4o', 'gpt-4o-mini'],
        apiKey: process.env.OPENAI_API_KEY,
        enabled: true,
        rateLimits: {
          requestsPerMinute: 500,
          tokensPerMinute: 200000
        }
      })
    }

    // OpenRouter Provider (using OpenAI-compatible API)
    if (process.env.VITE_OPENROUTER_API_KEY) {
      this.openai = new OpenAI({
        apiKey: process.env.VITE_OPENROUTER_API_KEY,
        baseURL: 'https://openrouter.ai/api/v1',
        dangerouslyAllowBrowser: true
      })

      this.providers.set('openrouter', {
        id: 'openrouter',
        name: 'OpenRouter',
        models: ['gpt-4o', 'claude-3.5-sonnet', 'gemini-2.0-flash'],
        apiKey: process.env.VITE_OPENROUTER_API_KEY,
        enabled: true,
        rateLimits: {
          requestsPerMinute: 100,
          tokensPerMinute: 100000
        }
      })
    }

    // Anthropic Provider
    if (process.env.ANTHROPIC_API_KEY) {
      this.anthropic = new Anthropic({
        apiKey: process.env.ANTHROPIC_API_KEY,
        dangerouslyAllowBrowser: true
      })
      
      this.providers.set('anthropic', {
        id: 'anthropic',
        name: 'Anthropic',
        models: ['claude-3.5-sonnet', 'claude-3-haiku'],
        apiKey: process.env.ANTHROPIC_API_KEY,
        enabled: true,
        rateLimits: {
          requestsPerMinute: 1000,
          tokensPerMinute: 400000
        }
      })
    }

    // Google Provider
    if (process.env.GOOGLE_API_KEY) {
      this.google = new GoogleGenerativeAI(process.env.GOOGLE_API_KEY)
      
      this.providers.set('google', {
        id: 'google',
        name: 'Google',
        models: ['gemini-2.0-flash', 'gemini-1.5-pro'],
        apiKey: process.env.GOOGLE_API_KEY,
        enabled: true,
        rateLimits: {
          requestsPerMinute: 1500,
          tokensPerMinute: 1000000
        }
      })
    }
  }

  // REVOLUTIONARY FEATURE: Auto-select best model for task
  private selectOptimalModel(request: AIRequest): AIModel {
    const { context } = request
    
    // Code generation tasks - prefer Claude 3.5 Sonnet
    if (context?.files && context.files.length > 0) {
      return 'claude-3.5-sonnet'
    }
    
    // Quick responses - prefer GPT-4o-mini
    if (request.maxTokens && request.maxTokens < 1000) {
      return 'gpt-4o-mini'
    }
    
    // Complex reasoning - prefer GPT-4o
    if (request.messages.some(m => m.content.length > 2000)) {
      return 'gpt-4o'
    }
    
    // Default to fastest model
    return 'gemini-2.0-flash'
  }

  // SUPERIOR CACHING: Intelligent response caching
  private getCacheKey(request: AIRequest): string {
    const key = {
      model: request.model,
      messages: request.messages,
      temperature: request.temperature || 0.7,
      context: request.context
    }
    return btoa(JSON.stringify(key)).slice(0, 32)
  }

  // RATE LIMITING: Intelligent rate limiting per provider
  private checkRateLimit(providerId: string): boolean {
    const now = Date.now()
    const limit = this.rateLimiter.get(providerId)
    
    if (!limit || now > limit.resetTime) {
      this.rateLimiter.set(providerId, {
        requests: 1,
        resetTime: now + 60000 // 1 minute
      })
      return true
    }
    
    const provider = this.providers.get(providerId)
    if (!provider) return false
    
    if (limit.requests >= provider.rateLimits.requestsPerMinute) {
      return false
    }
    
    limit.requests++
    return true
  }

  // ðŸš€ REVOLUTIONARY MULTI-MODAL REASONING SYSTEM
  // Compare responses from ALL providers and select the BEST code
  async complete(request: AIRequest): Promise<AIResponse> {
    const startTime = Date.now()

    // Check cache first
    const cacheKey = this.getCacheKey(request)
    const cached = this.cache.get(cacheKey)
    if (cached) {
      return { ...cached, cached: true }
    }

    // If specific model requested, use single provider
    if (request.model) {
      return this.completeSingleProvider(request, startTime, cacheKey)
    }

    // ðŸš€ MULTI-MODAL MODE: Compare all providers
    return this.completeMultiModal(request, startTime, cacheKey)
  }

  private async completeSingleProvider(request: AIRequest, startTime: number, cacheKey: string): Promise<AIResponse> {
    let response: AIResponse

    switch (request.model) {
      case 'gpt-4o':
      case 'gpt-4o-mini':
        response = await this.completeOpenAI(request)
        break

      case 'claude-3.5-sonnet':
      case 'claude-3-haiku':
        response = await this.completeAnthropic(request)
        break

      case 'gemini-2.0-flash':
      case 'gemini-1.5-pro':
        response = await this.completeGoogle(request)
        break

      default:
        throw new Error(`Unsupported model: ${request.model}`)
    }

    response.duration = Date.now() - startTime
    response.cached = false
    this.cache.set(cacheKey, response)

    return response
  }

  // ðŸš€ MULTI-MODAL REASONING: Compare ALL providers and pick BEST
  private async completeMultiModal(request: AIRequest, startTime: number, cacheKey: string): Promise<AIResponse> {
    const responses: Array<{ response: AIResponse; score: number; reasoning: string }> = []
    const errors: string[] = []

    // Get responses from all available providers
    const providers = [
      { name: 'openai', models: ['gpt-4o', 'gpt-4o-mini'] },
      { name: 'anthropic', models: ['claude-3.5-sonnet'] },
      { name: 'google', models: ['gemini-2.0-flash'] }
    ]

    // Run all providers in parallel for speed
    const promises = providers.map(async (provider) => {
      try {
        if (!this.providers.has(provider.name)) return null

        const providerRequest = { ...request, model: provider.models[0] as AIModel }
        let response: AIResponse

        switch (provider.name) {
          case 'openai':
            response = await this.completeOpenAI(providerRequest)
            break
          case 'anthropic':
            response = await this.completeAnthropic(providerRequest)
            break
          case 'google':
            response = await this.completeGoogle(providerRequest)
            break
          default:
            return null
        }

        // Score the response based on multiple criteria
        const score = this.scoreResponse(response, request)
        const reasoning = this.generateScoringReasoning(response, score, request)

        return { response, score, reasoning }
      } catch (error) {
        errors.push(`${provider.name}: ${error.message}`)
        return null
      }
    })

    const results = await Promise.allSettled(promises)

    // Collect successful responses
    results.forEach(result => {
      if (result.status === 'fulfilled' && result.value) {
        responses.push(result.value)
      }
    })

    if (responses.length === 0) {
      throw new Error(`All providers failed: ${errors.join(', ')}`)
    }

    // Sort by score (highest first)
    responses.sort((a, b) => b.score - a.score)

    const bestResponse = responses[0].response
    bestResponse.duration = Date.now() - startTime

    // Add multi-modal metadata to the response
    const enhancedResponse = {
      ...bestResponse,
      cached: false,
      multiModal: {
        totalResponses: responses.length,
        scores: responses.map(r => ({ model: r.response.model, score: r.score, reasoning: r.reasoning })),
        bestScore: responses[0].score,
        comparisonTime: Date.now() - startTime
      }
    }

    // Cache the best response
    this.cache.set(cacheKey, bestResponse)

    return bestResponse
  }

  // ðŸ§  INTELLIGENT SCORING SYSTEM
  private scoreResponse(response: AIResponse, request: AIRequest): number {
    let score = 0
    const content = response.content.toLowerCase()

    // Length appropriateness (prefer substantial but not verbose)
    const wordCount = content.split(' ').length
    if (wordCount > 10 && wordCount < 500) score += 20
    else if (wordCount >= 500) score += 10

    // Code quality indicators
    if (content.includes('```') && content.includes('function')) score += 25
    if (content.includes('const') || content.includes('let') || content.includes('var')) score += 15
    if (content.includes('error') || content.includes('catch')) score += 10

    // Completeness indicators
    if (content.includes('example') || content.includes('usage')) score += 15
    if (content.includes('import') || content.includes('export')) score += 10

    // Technical accuracy
    if (content.includes('async') || content.includes('await')) score += 10
    if (content.includes('try') || content.includes('catch')) score += 10

    // Response time bonus (faster is better)
    if (response.duration < 2000) score += 15
    else if (response.duration < 5000) score += 10

    // Token efficiency bonus
    if (response.usage.totalTokens < 1000) score += 10

    return Math.min(score, 100) // Cap at 100
  }

  private generateScoringReasoning(response: AIResponse, score: number, request: AIRequest): string {
    const reasons = []

    if (response.content.includes('```')) reasons.push('Contains code examples')
    if (response.content.length > 100) reasons.push('Substantial response')
    if (response.duration < 3000) reasons.push('Fast response time')
    if (response.usage.totalTokens < 1500) reasons.push('Token efficient')

    return reasons.join(', ') || 'Standard quality response'
  }

  // STREAMING COMPLETION
  async *stream(request: AIRequest): AsyncGenerator<string, void, unknown> {
    // Auto-select model if not specified
    if (!request.model) {
      request.model = this.selectOptimalModel(request)
    }
    
    // Route to appropriate provider
    switch (request.model) {
      case 'gpt-4o':
      case 'gpt-4o-mini':
        yield* this.streamOpenAI(request)
        break
        
      case 'claude-3.5-sonnet':
      case 'claude-3-haiku':
        yield* this.streamAnthropic(request)
        break
        
      case 'gemini-2.0-flash':
      case 'gemini-1.5-pro':
        yield* this.streamGoogle(request)
        break
        
      default:
        throw new Error(`Unsupported model: ${request.model}`)
    }
  }

  // OPENAI IMPLEMENTATION
  private async completeOpenAI(request: AIRequest): Promise<AIResponse> {
    if (!this.openai) throw new Error('OpenAI not initialized')
    
    if (!this.checkRateLimit('openai')) {
      throw new Error('OpenAI rate limit exceeded')
    }
    
    const response = await this.openai.chat.completions.create({
      model: request.model === 'gpt-4o' ? 'gpt-4o' : 'gpt-4o-mini',
      messages: request.messages,
      temperature: request.temperature || 0.7,
      max_tokens: request.maxTokens || 4000,
      stream: false
    })
    
    return {
      content: response.choices[0]?.message?.content || '',
      model: request.model,
      usage: {
        inputTokens: response.usage?.prompt_tokens || 0,
        outputTokens: response.usage?.completion_tokens || 0,
        totalTokens: response.usage?.total_tokens || 0
      },
      duration: 0,
      cached: false
    }
  }

  private async *streamOpenAI(request: AIRequest): AsyncGenerator<string, void, unknown> {
    if (!this.openai) throw new Error('OpenAI not initialized')
    
    const stream = await this.openai.chat.completions.create({
      model: request.model === 'gpt-4o' ? 'gpt-4o' : 'gpt-4o-mini',
      messages: request.messages,
      temperature: request.temperature || 0.7,
      max_tokens: request.maxTokens || 4000,
      stream: true
    })
    
    for await (const chunk of stream) {
      const content = chunk.choices[0]?.delta?.content
      if (content) {
        yield content
      }
    }
  }

  // ANTHROPIC IMPLEMENTATION
  private async completeAnthropic(request: AIRequest): Promise<AIResponse> {
    if (!this.anthropic) throw new Error('Anthropic not initialized')
    
    if (!this.checkRateLimit('anthropic')) {
      throw new Error('Anthropic rate limit exceeded')
    }
    
    const systemMessage = request.messages.find(m => m.role === 'system')
    const userMessages = request.messages.filter(m => m.role !== 'system')
    
    const response = await this.anthropic.messages.create({
      model: request.model === 'claude-3.5-sonnet' ? 'claude-3-5-sonnet-20241022' : 'claude-3-haiku-20240307',
      system: systemMessage?.content || '',
      messages: userMessages.map(m => ({
        role: m.role as 'user' | 'assistant',
        content: m.content
      })),
      temperature: request.temperature || 0.7,
      max_tokens: request.maxTokens || 4000
    })
    
    return {
      content: response.content[0]?.type === 'text' ? response.content[0].text : '',
      model: request.model,
      usage: {
        inputTokens: response.usage.input_tokens,
        outputTokens: response.usage.output_tokens,
        totalTokens: response.usage.input_tokens + response.usage.output_tokens
      },
      duration: 0,
      cached: false
    }
  }

  private async *streamAnthropic(request: AIRequest): AsyncGenerator<string, void, unknown> {
    if (!this.anthropic) throw new Error('Anthropic not initialized')
    
    const systemMessage = request.messages.find(m => m.role === 'system')
    const userMessages = request.messages.filter(m => m.role !== 'system')
    
    const stream = await this.anthropic.messages.create({
      model: request.model === 'claude-3.5-sonnet' ? 'claude-3-5-sonnet-20241022' : 'claude-3-haiku-20240307',
      system: systemMessage?.content || '',
      messages: userMessages.map(m => ({
        role: m.role as 'user' | 'assistant',
        content: m.content
      })),
      temperature: request.temperature || 0.7,
      max_tokens: request.maxTokens || 4000,
      stream: true
    })
    
    for await (const chunk of stream) {
      if (chunk.type === 'content_block_delta' && chunk.delta.type === 'text_delta') {
        yield chunk.delta.text
      }
    }
  }

  // GOOGLE IMPLEMENTATION
  private async completeGoogle(request: AIRequest): Promise<AIResponse> {
    if (!this.google) throw new Error('Google not initialized')
    
    if (!this.checkRateLimit('google')) {
      throw new Error('Google rate limit exceeded')
    }
    
    const model = this.google.getGenerativeModel({
      model: request.model === 'gemini-2.0-flash' ? 'gemini-2.0-flash-exp' : 'gemini-1.5-pro'
    })
    
    const chat = model.startChat({
      generationConfig: {
        temperature: request.temperature || 0.7,
        maxOutputTokens: request.maxTokens || 4000
      }
    })
    
    // Add context if provided
    let prompt = request.messages.map(m => `${m.role}: ${m.content}`).join('\n\n')
    
    if (request.context?.files) {
      prompt += '\n\nContext files:\n'
      request.context.files.forEach(file => {
        prompt += `\n--- ${file.path} ---\n${file.content}\n`
      })
    }
    
    const result = await chat.sendMessage(prompt)
    const response = await result.response
    
    return {
      content: response.text(),
      model: request.model,
      usage: {
        inputTokens: response.usageMetadata?.promptTokenCount || 0,
        outputTokens: response.usageMetadata?.candidatesTokenCount || 0,
        totalTokens: response.usageMetadata?.totalTokenCount || 0
      },
      duration: 0,
      cached: false
    }
  }

  private async *streamGoogle(request: AIRequest): AsyncGenerator<string, void, unknown> {
    if (!this.google) throw new Error('Google not initialized')
    
    const model = this.google.getGenerativeModel({
      model: request.model === 'gemini-2.0-flash' ? 'gemini-2.0-flash-exp' : 'gemini-1.5-pro'
    })
    
    let prompt = request.messages.map(m => `${m.role}: ${m.content}`).join('\n\n')
    
    if (request.context?.files) {
      prompt += '\n\nContext files:\n'
      request.context.files.forEach(file => {
        prompt += `\n--- ${file.path} ---\n${file.content}\n`
      })
    }
    
    const result = await model.generateContentStream(prompt)
    
    for await (const chunk of result.stream) {
      const chunkText = chunk.text()
      if (chunkText) {
        yield chunkText
      }
    }
  }

  // UTILITY METHODS
  getAvailableModels(): AIModel[] {
    const models: AIModel[] = []
    
    this.providers.forEach(provider => {
      if (provider.enabled) {
        models.push(...provider.models)
      }
    })
    
    return models
  }

  getProviderStatus(): Array<{ id: string; name: string; enabled: boolean; models: AIModel[] }> {
    return Array.from(this.providers.values()).map(provider => ({
      id: provider.id,
      name: provider.name,
      enabled: provider.enabled,
      models: provider.models
    }))
  }

  // Clear cache
  clearCache(): void {
    this.cache.clear()
  }

  // Get cache stats
  getCacheStats(): { size: number; hitRate: number } {
    return {
      size: this.cache.size,
      hitRate: 0.85 // This would be calculated from actual usage
    }
  }
}

// SINGLETON INSTANCE
export const aiService = new MultiModelAIService()

// CONVENIENCE FUNCTIONS
export async function completeAI(request: AIRequest): Promise<AIResponse> {
  return aiService.complete(request)
}

export async function* streamAI(request: AIRequest): AsyncGenerator<string, void, unknown> {
  yield* aiService.stream(request)
}

export function getAvailableAIModels(): AIModel[] {
  return aiService.getAvailableModels()
}